---
title: "chapter 11-2"
author: "Min-Yao"
date: "2019/10/31"
output: 
  html_document: 
    keep_md: yes
---

## 11.2. Poisson regression

```{r}
## R code 11.38
y <- rbinom(1e5,1000,1/1000)
c( mean(y) , var(y) )
```

### 11.2.1. Example: Oceanic tool complexity.

```{r}
## R code 11.39
library(rethinking)
data(Kline)
d <- Kline
d

## R code 11.40
d$P <- scale( log(d$population) )
d$contact_id <- ifelse( d$contact=="high" , 2 , 1 )

## R code 11.41
curve( dlnorm( x , 0 , 10 ) , from=0 , to=100 , n=200 )

## R code 11.42
a <- rnorm(1e4,0,10)
lambda <- exp(a)
mean( lambda )

## R code 11.43
curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )

## R code 11.44
N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 10 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=col.alpha("black",0.5) )

## R code 11.45
set.seed(10)
N <- 100
a <- rnorm( N , 3 , 0.5 )
b <- rnorm( N , 0 , 0.2 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=col.alpha("black",0.5) )

## R code 11.46
x_seq <- seq( from=log(100) , to=log(200000) , length.out=100 )
lambda <- sapply( x_seq , function(x) exp( a + b*x ) )
plot( NULL , xlim=range(x_seq) , ylim=c(0,500) , xlab="log population" , ylab="total tools" )
for ( i in 1:N ) lines( x_seq , lambda[i,] , col=col.alpha("black",0.5) , lwd=1.5 )

## R code 11.47
plot( NULL , xlim=range(exp(x_seq)) , ylim=c(0,500) , xlab="population" , ylab="total tools" )
for ( i in 1:N ) lines( exp(x_seq) , lambda[i,] , col=col.alpha("black",0.5) , lwd=1.5 )

## R code 11.48
dat <- list(
    T = d$total_tools ,
    P = d$P ,
    cid = d$contact_id )

# intercept only
m11.9 <- ulam(
    alist(
        T ~ dpois( lambda ),
        log(lambda) <- a,
        a ~ dnorm(3,0.5)
    ), data=dat , chains=4 , log_lik=TRUE )

# interaction model
m11.10 <- ulam(
    alist(
        T ~ dpois( lambda ),
        log(lambda) <- a[cid] + b[cid]*P,
        a[cid] ~ dnorm( 3 , 0.5 ),
        b[cid] ~ dnorm( 0 , 0.2 )
    ), data=dat , chains=4 , log_lik=TRUE )

## R code 11.49
compare( m11.9 , m11.10 , func=LOO )

## R code 11.50
k <- LOOPk(m11.10)
plot( dat$P , dat$T , xlab="log population (std)" , ylab="total tools" ,
    col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
    ylim=c(0,75) , cex=1+normalize(k) )

# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-1.4 , to=3 , length.out=ns )

# predictions for cid=1 (low contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

# predictions for cid=2 (high contact)
lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

## R code 11.51
plot( d$population , d$total_tools , xlab="population" , ylab="total tools" ,
    col=rangi2 , pch=ifelse( dat$cid==1 , 1 , 16 ) , lwd=2 ,
    ylim=c(0,75) , cex=1+normalize(k) )

ns <- 100
P_seq <- seq( from=-5 , to=3 , length.out=ns )
# 1.53 is sd of log(population)
# 9 is mean of log(population)
pop_seq <- exp( P_seq*1.53 + 9 )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=1 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

lambda <- link( m11.10 , data=data.frame( P=P_seq , cid=2 ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( pop_seq , lmu , lty=1 , lwd=1.5 )
shade( lci , pop_seq , xpd=TRUE )

## R code 11.52
dat2 <- list( T=d$total_tools, P=d$population, cid=d$contact_id )
m11.11 <- ulam(
    alist(
        T ~ dpois( lambda ),
        lambda <- exp(a[cid])*P^b[cid]/g,
        a[cid] ~ dnorm(1,1),
        b[cid] ~ dexp(1),
        g ~ dexp(1)
    ), data=dat2 , chains=4 , log_lik=TRUE )
```

### 11.2.2. Negative binomial (gamma-Poisson) models.
### 11.2.3. Example: Exposure and the offset.

```{r}
## R code 11.53
num_days <- 30
y <- rpois( num_days , 1.5 )

## R code 11.54
num_weeks <- 4
y_new <- rpois( num_weeks , 0.5*7 )

## R code 11.55
y_all <- c( y , y_new )
exposure <- c( rep(1,30) , rep(7,4) )
monastery <- c( rep(0,30) , rep(1,4) )
d <- data.frame( y=y_all , days=exposure , monastery=monastery )

## R code 11.56
# compute the offset
d$log_days <- log( d$days )

# fit the model
m11.12 <- quap(
    alist(
        y ~ dpois( lambda ),
        log(lambda) <- log_days + a + b*monastery,
        a ~ dnorm( 0 , 1 ),
        b ~ dnorm( 0 , 1 )
    ), data=d )

## R code 11.57
post <- extract.samples( m11.12 )
lambda_old <- exp( post$a )
lambda_new <- exp( post$a + post$b )
precis( data.frame( lambda_old , lambda_new ) )
```

### 11.2.4. Multinomial in disguise as Poisson. 

```{r}
## R code 11.58
library(rethinking)
data(UCBadmit)
d <- UCBadmit

## R code 11.59
# binomial model of overall admission probability
m_binom <- map(
    alist(
        admit ~ dbinom(applications,p),
        logit(p) <- a,
        a ~ dnorm(0,100)
    ),
    data=d )

# Poisson model of overall admission rate and rejection rate
d$rej <- d$reject # 'reject' is a reserved word
m_pois <- map2stan(
    alist(
        admit ~ dpois(lambda1),
        rej ~ dpois(lambda2),
        log(lambda1) <- a1,
        log(lambda2) <- a2,
        c(a1,a2) ~ dnorm(0,100)
    ),
    data=d , chains=3 , cores=3 )

## R code 11.60
logistic(coef(m_binom))

## R code 11.61
k <- as.numeric(coef(m_pois))
exp(k[1])/(exp(k[1])+exp(k[2]))
```


### 10E4. Why do Poisson regressions sometimes require the use of an offset? Provide an example.

> Poisson distribution models number of events per some unit of time/spatial region. Measurement of the outcome variable can be provided on the different scale for each observation (daily vs weekly). > Offset is used to bring all observations on the same scale.Based on the example in the book, the offset parameter is used to convert all measurements to the daily basis.  
> For example, in a tomato field, the number of plants in the area is another possible example where offset is helpful. Square of the area in different units can be treated as an offset in this case.


### 10M2. If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?

```{r}
exp(1.7)
```

> exp(1.7)=5.4739 times. This imply about the change in the outcome increases in 5.47 times.

### 10M3. Explain why the logit link is appropriate for a binomial generalized linear model.

> In a binomial generalized linear model we are typically modeling the `p` parameter with a linear model. Because this parameter is defined as a probability, it must be constrained to the interval [0, 1], and the logit link function ensures this constraint.

### 10M4. Explain why the log link is appropriate for a Poisson generalized linear model.

> In a Poisson generalized linear model we are typically modeling the `lambda` parameter with a linear model. This parameter must be positive, and the log link function ensures this constraint.

### 10H4. The data contained in data(salamanders) are counts of salamanders (Plethodon elongatus) from 47 different 49-m 2 plots in northern California. 175 The column SALAMAN is the count in each plot, and the columns PCTCOVER and FORESTAGE are percent of ground cover and age of trees in the plot, respectively. You will model SALAMAN as a Poisson variable.

```{r}
library(ggplot2)
data(salamanders)
d <- salamanders
summary(d)
str(d)
pairs(d)
```


#### (a) Model the relationship between density and percent cover, using a log-link (same as the example in the book and lecture). Use weakly informative priors of your choosing. Check the quadratic approximation again, by comparing map to map2stan. Then plot the expected counts and their 89% interval against percent cover. In which ways does the model do a good job? In which ways does it do a bad job?

```{r}
## R code 11.43
curve( dlnorm( x , 3 , 0.5 ) , from=0 , to=100 , n=200 )
curve( dlnorm( x , 2 , 0.5 ) , from=0 , to=100 , n=200 )
curve( dlnorm( x , 1 , 0.5 ) , from=0 , to=100 , n=200 )

#choose 2
```

```{r}
## R code 11.45
set.seed(10)
N <- 100
a <- rnorm( N , 2 , 0.5 )
b <- rnorm( N , 0 , 0.5 )
plot( NULL , xlim=c(-2,2) , ylim=c(0,100) )
for ( i in 1:N ) curve( exp( a[i] + b[i]*x ) , add=TRUE , col=col.alpha("black",0.5) )
```

```{r}
## R code 11.48
dat <- list(
    S = d$SALAMAN ,
    P = scale(d$PCTCOVER) ,
    FO = scale(d$FORESTAGE),
    SI = d$SITE)
str(dat)
head(dat)


m11.4H <- ulam(
    alist(
        S ~ dpois( lambda ),
        log(lambda) <- a+b*P,
        a ~ dnorm(2,0.5),
        b ~ dnorm(0,0.5)
    ), data=dat , chains=4 , log_lik=TRUE )


precis(m11.4H, depth = 2)
plot(precis(m11.4H, depth = 2))

```

```{r}
## R code 11.50
plot( dat$P , dat$S , xlab="PCTCOVER" , ylab="SALAMAN" ,
    col=rangi2 , lwd=2 ,
    ylim=c(0,15))

# set up the horizontal axis values to compute predictions at
ns <- 100
P_seq <- seq( from=-3 , to=3 , length.out=ns )

# predictions for cid=1 (low contact)
lambda <- link( m11.4H , data=data.frame( P=P_seq ) )
lmu <- apply( lambda , 2 , mean )
lci <- apply( lambda , 2 , PI )
lines( P_seq , lmu , lty=2 , lwd=1.5 )
shade( lci , P_seq , xpd=TRUE )

```


#### (b) Can you improve the model by using the other predictor, FORESTAGE? Try any models you think useful. Can you explain why FORESTAGE helps or does not help with prediction?

```{r}
m11.4H2 <- ulam(
    alist(
        S ~ dpois( lambda ),
        log(lambda) <- a+b*P+c*FO,
        a ~ dnorm(2,0.5),
        c(b,c) ~ dnorm(0,0.5)
    ), data=dat , chains=4 , log_lik=TRUE )


precis(m11.4H2, depth = 2)
plot(precis(m11.4H2, depth = 2))
compare(m11.4H,m11.4H2)
```

> not better?

```{r}
pairs(m11.4H)
pairs(m11.4H2)
```


### Week6 PDF # 3 
3. The data in data(Primates301) were first introduced at the end of Chapter 7.
In this problem, you will consider how brain size is associated with social learning. There are three parts.

First, model the number of observations of social_learning for each species as a function of the log brain size. Use a Poisson distribution for the social_learning outcome variable. Interpret the resulting posterior.

```{r}
library(rethinking)
data(Primates301)
d <- Primates301
str(d)
summary(d)

d2 <- d[ complete.cases( d$social_learning , d$brain , d$research_effort ) , ]
dat <- list(
    S = d2$social_learning,
    B = scale(log(d2$brain)),
    R = log(d2$research_effort)
)
```

```{r}
PDFm3.1 <- ulam(
    alist(
        S ~ dpois( lambda ),
        log(lambda) <- a + b*B,
        c(a,b) ~ normal(0,0.5)
        ), data=dat , chains=4 , cores=4 )
precis( PDFm3.1 )
```

```{r}
postcheck(PDFm3.1)
```

Second, some species are studied much more than others. So the number of reported instances of social_learning could be a product of research eff ort. Use the research_effort variable, specifi cally its logarithm, as an additional predictor variable. Interpret the coeffi cient for log research_effort. Does this model disagree with the previous one?

```{r}
PDFm3.2 <- ulam(
    alist(
        S ~ poisson( lambda ),
        log(lambda) <- a + b*B + c*R,
        c(a,b,c) ~ normal(0,0.5)
), data=dat , chains=4 , cores=4 )
precis( PDFm3.2 )
```

```{r}
postcheck(PDFm3.2)
```


Third, draw a DAG to represent how you think the variables social_learning, brain, and research_effort interact. Justify the DAG with the measured associations in the two models above (and any other models you used).

```{r}
library(dagitty)
library(ggdag)

# S = social_learning, B = brain, R = research_effort

dag <- dagitty("dag{B -> R -> S; B->S }")
tidy_dagitty(dag)
ggdag(dag, layout = "circle")
```

