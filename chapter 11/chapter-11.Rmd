---
title: "chapter 11"
author: "Min-Yao"
date: "2019/10/28"
output: html_document
---

# 11. God Spiked the Integers

## 11.1. Binomial regression

### 11.1.1. Logistic regression: Prosocial chimpanzees.


```{r}
## R code 11.1
library(rethinking)
data(chimpanzees)
d <- chimpanzees

## R code 11.2
d$treatment <- 1 + d$prosoc_left + 2*d$condition

## R code 11.3
xtabs( ~ treatment + prosoc_left + condition , d )

## R code 11.4
m11.1 <- quap(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a ,
        a ~ dnorm( 0 , 10 )
    ) , data=d )

## R code 11.5
set.seed(1999)
prior <- extract.prior( m11.1 , n=1e4 )

## R code 11.6
p <- inv_logit( prior$a )
dens( p , adj=0.1 )
```


```{r}
## R code 11.7
m11.2 <- quap(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a + b[treatment] ,
        a ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 10 )
    ) , data=d )
set.seed(1999)
prior <- extract.prior( m11.2 , n=1e4 )
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )

## R code 11.8
dens( abs( p[,1] - p[,2] ) , adj=0.1 )
```


```{r}
## R code 11.9
m11.3 <- quap(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a + b[treatment] ,
        a ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 0.5 )
    ) , data=d )
set.seed(1999)
prior <- extract.prior( m11.3 , n=1e4 )
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )
mean( abs( p[,1] - p[,2] ) )

## R code 11.8
dens( abs( p[,1] - p[,2] ) , adj=0.1 )
```


```{r}
## R code 11.10
# prior trimmed data list
dat_list <- list(
    pulled_left = d$pulled_left,
    actor = d$actor,
    treatment = as.integer(d$treatment) )

# particles in 11-dimensional space
m11.4 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + b[treatment] ,
        a[actor] ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 0.5 )
    ) ,
    data=dat_list , chains=4 )
precis( m11.4 , depth=2 )
```

```{r}
## R code 11.11
post <- extract.samples(m11.4)
p_left <- inv_logit( post$a )
plot( precis( as.data.frame(p_left) ) , xlim=c(0,1) )
```


```{r}
## R code 11.12
labs <- c("R/N","L/N","R/P","L/P")
plot( precis( m11.4 , depth=2 , pars="b" ) , labels=labs )
```


```{r}
## R code 11.13
diffs <- list(
    db13 = post$b[,1] - post$b[,3],
    db24 = post$b[,2] - post$b[,4] )
plot( precis(diffs) )
```


```{r}
## R code 11.14
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean )
pl[1,]
```

```{r}
## R code 11.15
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
    ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
for ( j in (1:7)[-2] ) {
    lines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
    lines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28 , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.01
text( 1 , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2 , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3 , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4 , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
mtext( "observed proportions\n" )
```

```{r}
## R code 11.16
dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link_ulam( m11.4 , data=dat )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )

## R code 11.17
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2

## R code 11.18
dat_list2 <- list(
    pulled_left = d$pulled_left,
    actor = d$actor,
    side = d$side,
    cond = d$cond )
m11.5 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + bs[side] + bc[cond] ,
        a[actor] ~ dnorm( 0 , 1.5 ),
        bs[side] ~ dnorm( 0 , 0.5 ),
        bc[cond] ~ dnorm( 0 , 0.5 )
    ) ,
    data=dat_list2 , chains=4 , log_lik=TRUE )

m11.4 <- ulam(
    alist(
        pulled_left ~ dbinom( 1 , p ) ,
        logit(p) <- a[actor] + b[treatment] ,
        a[actor] ~ dnorm( 0 , 1.5 ),
        b[treatment] ~ dnorm( 0 , 0.5 )
    ) ,
    data=dat_list , chains=4 , log_lik=TRUE)

## R code 11.19
compare( m11.5 , m11.4 , func=LOO )
```

```{r}
## R code 11.20
post <- extract.samples( m11.4 , clean=FALSE )
str(post)

## R code 11.21
m11.4_stan_code <- stancode(m11.4)
m11.4_stan <- stan( model_code=m11.4_stan_code , data=dat_list , chains=4 )
compare( m11.4_stan , m11.4 )
```

### 11.1.2. Relative shark and absolute penguin.

```{r}
## R code 11.22
post <- extract.samples(m11.4)
mean( exp(post$b[,4]-post$b[,2]) )
```

### 11.1.3. Aggregated binomial: Chimpanzees again, condensed.

```{r}
## R code 11.23
data(chimpanzees)
d <- chimpanzees
d$treatment <- 1 + d$prosoc_left + 2*d$condition
d$side <- d$prosoc_left + 1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
    d$pulled_left ,
    list( treatment=d$treatment , actor=d$actor ,
          side=d$side , cond=d$cond ) ,
    sum )
colnames(d_aggregated)[5] <- "left_pulls"

## R code 11.24
dat <- with( d_aggregated , list(
    left_pulls = left_pulls,
    treatment = treatment,
    actor = actor,
    side = side,
    cond = cond ) )

m11.6 <- ulam(
    alist(
        left_pulls ~ dbinom( 18 , p ) ,
        logit(p) <- a[actor] + b[treatment] ,
        a[actor] ~ dnorm( 0 , 1.5 ) ,
        b[treatment] ~ dnorm( 0 , 0.5 )
    ) ,
    data=dat , chains=4 , log_lik=TRUE )

## R code 11.25
compare( m11.6 , m11.4 , func=LOO )

## R code 11.26
# deviance of aggregated 6-in-9
-2*dbinom(6,9,0.2,log=TRUE)
# deviance of dis-aggregated
-2*sum(dbern(c(1,1,1,1,1,1,0,0,0),0.2,log=TRUE))

## R code 11.27
( k <- LOOPk(m11.6) )

```


### 11.1.4. Aggregated binomial: Graduate school admissions.

```{r}
## R code 11.28
library(rethinking)
data(UCBadmit)
d <- UCBadmit

## R code 11.29
d$gid <- ifelse( d$applicant.gender=="male" , 1 , 2 )
m11.7 <- quap(
    alist(
        admit ~ dbinom( applications , p ) ,
        logit(p) <- a[gid] ,
        a[gid] ~ dnorm( 0 , 1.5 )
    ) , data=d )
precis( m11.7 , depth=2 )
```

```{r}
## R code 11.30
post <- extract.samples(m11.7)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```


```{r}
## R code 11.31
postcheck( m11.7 , n=1e4 )
# draw lines connecting points from same dept
d$dept_id <- rep( 1:6 , each=2 )
for ( i in 1:6 ) {
    x <- 1 + 2*(i-1)
    y1 <- d$admit[x]/d$applications[x]
    y2 <- d$admit[x+1]/d$applications[x+1]
    lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
    text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}
```

```{r}
## R code 11.32
d$dept_id <- rep(1:6,each=2)
m11.8 <- quap(
    alist(
        admit ~ dbinom( applications , p ) ,
        logit(p) <- a[gid] + delta[dept_id] ,
        a[gid] ~ dnorm( 0 , 1.5 ) ,
        delta[dept_id] ~ dnorm( 0 , 1.5 )
    ) , data=d )
precis( m11.8 , depth=2 )

## R code 11.33
post <- extract.samples(m11.8)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

```{r}
## R code 11.34
pg <- sapply( 1:6 , function(k)
    d$applications[d$dept_id==k]/sum(d$applications[d$dept_id==k]) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )
```

### 11.1.5. Multinomial and categorical models. 

```{r}
## R code 11.35
# simulate career choices among 500 individuals
N <- 500             # number of individuals
income <- 1:3        # expected income of each career
score <- 0.5*income  # scores for each career, based on income
# next line converts scores to probabilities
p <- softmax(score[1],score[2],score[3])

# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA,N)  # empty vector of choices for each individual
# sample chosen career for each individual
for ( i in 1:N ) career[i] <- sample( 1:3 , size=1 , prob=p )

## R code 11.36
# fit the model, using dcategorical and softmax link
m10.16 <- map(
    alist(
        career ~ dcategorical( softmax(0,s2,s3) ),
        s2 <- b*2,    # linear model for event type 2
        s3 <- b*3,    # linear model for event type 3
        b ~ dnorm(0,5)
    ) ,
    data=list(career=career) )

## R code 11.37
N <- 100
# simulate family incomes for each individual
family_income <- runif(N)
# assign a unique coefficient for each type of event
b <- (1:-1)
career <- rep(NA,N)  # empty vector of choices for each individual
for ( i in 1:N ) {
    score <- 0.5*(1:3) + b*family_income[i]
    p <- softmax(score[1],score[2],score[3])
    career[i] <- sample( 1:3 , size=1 , prob=p )
}

m10.17 <- map(
    alist(
        career ~ dcategorical( softmax(0,s2,s3) ),
        s2 <- a2 + b2*family_income,
        s3 <- a3 + b3*family_income,
        c(a2,a3,b2,b3) ~ dnorm(0,5)
    ) ,
    data=list(career=career,family_income=family_income) )
```


## 11.4. Summary

## 11.5. Practice

### Easy.

#### 10E1. If an event has probability 0.35, what are the log-odds of this event?

```{r}
p <- 0.35
p/(1-p)
```


#### 10E2. If an event has log-odds 3.2, what is the probability of this event?

$\ L = p/(1-p)$
$\ L(1-p) = p$
$\ L-L*p = p$
$\ L = p + L*p$
$\ L = p(1+L)$
$\ p = L/(1+L)$


```{r}
logit <- 3.2
logit/(1+logit)
```


#### 10E3. Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?

$\ logit(p)=log(p/(1-p))=1.7$

$\ p/(1-p)=exp(1.7)$

```{r}
exp(1.7)
```


### Medium.

#### 10M1. As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?

> The major reason is the the aggregated model contains an extra factor in its log-probabilities, because of the way the data are organized.

> if set a = successes, b = trials

likelihood in the aggregated format: $\Pr(a|b,p) = [a!/a!*(b-a)!]*p^a*(1-p)^{b-a}$

likelihood in the non-aggregated format: $\ Pr(1,1,...,0,...|p) = p^a*(1-p)^{b-a}$

#### 1. The data in data(NWOGrants) are outcomes for scientific funding applications for the Netherlands Organization for Scientific Research (NWO) from 2010–2012 (see van der Lee and Ellemers doi:10.1073/pnas.1510159112). These data have a very similar structure to the UCBAdmit data discussed in Chapter 11. I want you to consider a similar question: What are the total and indirect causal effects of gender on grant awards? Consider a mediation path (a pipe) through discipline. Draw the corresponding DAG and then use one or more binomial GLMs to answer the question. What is your causal interpretation? If NWO’s goal is to equalize rates of funding between the genders, what type of intervention would be most effective?

```{r}
library(rethinking)
data(NWOGrants)

data_NWOGrants <- NWOGrants
summary(data_NWOGrants)
str(data_NWOGrants)
head(data_NWOGrants)
```

```{r}
library(dagitty)
library(ggdag)

# A = awards, G = gender, D = discipline

dag <- dagitty("dag{G -> D -> A; G->A }")
tidy_dagitty(dag)
ggdag(dag, layout = "circle")
```


#### overall

```{r}
data_list1 <- list(
    awards = as.integer(data_NWOGrants$awards),
    applications = as.integer(data_NWOGrants$applications),
    gender = as.integer(data_NWOGrants$gender) # male=2, female=1
    )
summary(data_list1)
str(data_list1)
head(data_list1)

m11.1_total <- ulam(
    alist(
        awards ~ dbinom( applications , p ),
        logit(p) <- a[gender],
        a[gender] ~ normal(0,1.5)), 
    data=data_list1 , chains=4 , cores = 4)

precis(m11.1_total,2)
```

```{r}
## R code 11.30
post <- extract.samples(m11.1_total)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )
```

> relative: women are 20% less likely to receive the award.

> absolute: women are 3% less likely to receive the award.

#### add discipline

```{r}
data_list2 <- list(
    awards = as.integer(data_NWOGrants$awards),
    applications = as.integer(data_NWOGrants$applications),
    gender = as.integer(data_NWOGrants$gender), # male=2, female=1
    discipline = as.integer(data_NWOGrants$discipline)
    )
summary(data_list2)
str(data_list2)
head(data_list2)

m11.1_dis <- ulam(
    alist(
        awards ~ dbinom( applications , p ),
        logit(p) <- a[gender] + b[discipline],
        a[gender] ~ normal(0,1.5),
        b[discipline] ~ dnorm(0,1.5)), 
    data=data_list2 , chains=4 , cores = 4)

precis(m11.1_dis,2)
```

```{r}
## R code 11.30
post <- extract.samples(m11.1_dis)
diff_a <- post$a[,1] - post$a[,2]
diff_p <- inv_logit(post$a[,1]) - inv_logit(post$a[,2])
precis( list( diff_a=diff_a , diff_p=diff_p ) )

## R code 11.31
postcheck( m11.1_dis , n=1e4 )

```

> relative: women are 14% less likely to receive the award.

> absolute: women are 2% less likely to receive the award.

> The difference reduced, but Women still have less chance to receive the award. Based on the figure, it seems like some disciplines have relative large number differences between male and female. If NWO’s goal is to equalize rates of funding between the genders, they should investigate why these specific disciplines have these differences. In addition, there may be also other factors influence the differences. like race, age, their research institute... etc. 

#### 2. Suppose that the NWO Grants sample has an unobserved confound that influences both choice of discipline and the probability of an award. One example of such a confound could be the career stage of each applicant. Suppose that in some disciplines, junior scholars apply for most of the grants. In other disciplines, scholars from all career stages compete. As a result, career stage influences discipline as well as the probability of being awarded a grant. Add these influences to your DAG from Problem 1. What happens now when you condition on discipline? Does it provide an un-confounded estimate of the direct path from gender to an award? Why or why not? Justify your answer with the back-door criterion. Hint: This is structurally a lot like the grandparents-parents-children-neighborhoods example from a preen analyze it using the model from Problem 1. What do you conclude? Is it possible for gender to have a real direct causal influence but for a regression conditioning on both gender and discipline to suggest zero influence?

```{r}
library(dagitty)
library(ggdag)

# A = awards, G = gender, D = discipline, C = career stage

dag <- dagitty("dag{G -> D -> A; G -> A; D <- C -> A}")
tidy_dagitty(dag)
ggdag(dag, layout = "circle")
```

> If I condition on discipline, there is still a back door from G -> D -> C -> A. Like the grandparents-parents-children-neighborhoods example, Now D is a common consequence of G and C, so if we condition on D, it will bias inference about G -> A. Let's use the code from chapter 6 to do the simulation.

```{r}
## R code 6.26
N <- 200  # number of applicants
b_GD <- 1 # direct effect of G on D
b_GA <- 0 # direct effect of G on A
b_DA <- 1 # direct effect of D on A
b_C <- 2  # direct effect of C on D and A

library(Rlab)

## R code 6.27
set.seed(1)
C <- 2*rbern( N , 0.5 ) - 1 # Random Sample From Bernoulli Distribution
G <- rnorm( N )
D <- rnorm( N , b_GD*G + b_C*C )
A <- rnorm( N , b_DA*D + b_GA*G + b_C*C )
d <- data.frame( A=A , D=D , G=G , C=C )
summary(d)
str(d)
head(d)
```

>  I’ve assumed that gender G have zero effect on their awards A.

```{r}
## R code 6.28
m11.2 <- ulam(
    alist(
        A ~ dnorm( mu , sigma ),
        mu <- a + b_DA*D + b_GA*G,
        a ~ dnorm( 0 , 1 ),
        c(b_DA,b_GA) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d , chains=4 , cores=4)
precis(m11.2)

## R code 6.29
m11.2_C <- ulam(
    alist(
        A ~ dnorm( mu , sigma ),
        mu <- a + b_DA*D + b_GA*G + b_C*C,
        a ~ dnorm( 0 , 1 ),
        c(b_DA,b_GA,b_C) ~ dnorm( 0 , 1 ),
        sigma ~ dexp( 1 )
    ), data=d , chains=4 , cores=4)
precis(m11.2_C)
```

> Simpson’s paradox! The unmeasured C makes D a collider, and conditioning on D produces collider bias, so we have to include C.